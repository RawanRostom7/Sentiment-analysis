{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUiwkRv-XAkW"
      },
      "source": [
        "# Download MARBERT checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8wBRd5zXAkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01d14a1-3776-46fb-cb2e-66a4f614f741"
      },
      "source": [
        "!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-16 22:27:03--  https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.114, 3.163.189.74, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27MARBERT_pytorch_verison.tar.gz%3B+filename%3D%22MARBERT_pytorch_verison.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1718836023&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODgzNjAyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9VQkMtTkxQL01BUkJFUlQvODViZmVjNzZmMzhjYmE0YmMyZTZjZDAyYTk1OTAxNmRlMzdiYTkzZGU0Yzg1MGE3ZDE3NTgxMWRjZTRlOGFkYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=HOSMcKWmMRmGjmnIC99Fln50fHGuaK-pmA5upR93zOoJBMqe1zvEywrXfi9fYPR%7EXEFLuT5agPzVjbRiiQv1Rkoqo2%7EJpTfdx9K1sx6%7E0gC-UhINgTZJc120apYXJCLQCSofGOJZWP817a3tAHxCj0OZ-d23X9Frh1WtkoW-vvrH9pKI-gRoHxKilHhT0bIaCriHlvM0Oacn-f3Vu4A7emf83H8jjcvoAr-4DhRqgdc1%7ELBKw3IfWD6GMInWy71B63MzIgUx92jy2LeoMVZqVMUHrEwJqnpKjU4HcColIcTwQLVLvG6FnCFC2cPcgfWbUUhCNDA-Xu4lC%7EznOe-5Uw__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-06-16 22:27:03--  https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27MARBERT_pytorch_verison.tar.gz%3B+filename%3D%22MARBERT_pytorch_verison.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1718836023&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODgzNjAyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9VQkMtTkxQL01BUkJFUlQvODViZmVjNzZmMzhjYmE0YmMyZTZjZDAyYTk1OTAxNmRlMzdiYTkzZGU0Yzg1MGE3ZDE3NTgxMWRjZTRlOGFkYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=HOSMcKWmMRmGjmnIC99Fln50fHGuaK-pmA5upR93zOoJBMqe1zvEywrXfi9fYPR%7EXEFLuT5agPzVjbRiiQv1Rkoqo2%7EJpTfdx9K1sx6%7E0gC-UhINgTZJc120apYXJCLQCSofGOJZWP817a3tAHxCj0OZ-d23X9Frh1WtkoW-vvrH9pKI-gRoHxKilHhT0bIaCriHlvM0Oacn-f3Vu4A7emf83H8jjcvoAr-4DhRqgdc1%7ELBKw3IfWD6GMInWy71B63MzIgUx92jy2LeoMVZqVMUHrEwJqnpKjU4HcColIcTwQLVLvG6FnCFC2cPcgfWbUUhCNDA-Xu4lC%7EznOe-5Uw__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.161.6.126, 18.161.6.107, 18.161.6.100, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.161.6.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607066087 (579M) [application/gzip]\n",
            "Saving to: ‘MARBERT_pytorch_verison.tar.gz’\n",
            "\n",
            "MARBERT_pytorch_ver 100%[===================>] 578.94M   199MB/s    in 2.9s    \n",
            "\n",
            "2024-06-16 22:27:06 (199 MB/s) - ‘MARBERT_pytorch_verison.tar.gz’ saved [607066087/607066087]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FYvgJevXAkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f69e45c-14d4-433b-837d-cf948b34d971"
      },
      "source": [
        "!tar -xvf MARBERT_pytorch_verison.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MARBERT_pytorch_verison/\n",
            "MARBERT_pytorch_verison/pytorch_model.bin\n",
            "MARBERT_pytorch_verison/config.json\n",
            "MARBERT_pytorch_verison/vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUeiBs3tXAkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee2c5ae-7a78-475d-aff8-2469174bd1c5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_test.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-16 22:27:15--  https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149495 (146K) [text/plain]\n",
            "Saving to: ‘UBC_AJGT_final_shuffled_train.tsv’\n",
            "\n",
            "UBC_AJGT_final_shuf 100%[===================>] 145.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-06-16 22:27:15 (8.02 MB/s) - ‘UBC_AJGT_final_shuffled_train.tsv’ saved [149495/149495]\n",
            "\n",
            "--2024-06-16 22:27:16--  https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36901 (36K) [text/plain]\n",
            "Saving to: ‘UBC_AJGT_final_shuffled_test.tsv’\n",
            "\n",
            "UBC_AJGT_final_shuf 100%[===================>]  36.04K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2024-06-16 22:27:16 (4.98 MB/s) - ‘UBC_AJGT_final_shuffled_test.tsv’ saved [36901/36901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXM2ZsS1XAka"
      },
      "source": [
        "!mkdir -p AJGT\n",
        "!mv UBC_AJGT_final_shuffled_train.tsv ./AJGT/UBC_AJGT_final_shuffled_train.tsv\n",
        "!mv UBC_AJGT_final_shuffled_test.tsv ./AJGT/UBC_AJGT_final_shuffled_test.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2pkLcyfXAka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d76e7bc-7ab7-4e4d-d1d2-6f97fbcf7c7d"
      },
      "source": [
        "!pip install GPUtil pytorch_pretrained_bert transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.25.2)\n",
            "Collecting boto3 (from pytorch_pretrained_bert)\n",
            "  Downloading boto3-1.34.127-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2024.5.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch_pretrained_bert)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.127 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading botocore-1.34.127-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.127->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.127->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=2f44b70a44db53fe12a758abdb09a201373559b17b79317065ceda15434fc1d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_pretrained_bert\n",
            "Successfully installed GPUtil-1.4.0 boto3-1.34.127 botocore-1.34.127 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch_pretrained_bert-0.6.2 s3transfer-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBm7opGXAkb"
      },
      "source": [
        "# Fine-tuning code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oOXwzeXXAkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3ce42c-a7ee-4ede-fbcb-3043d918f0a1"
      },
      "source": [
        "# (1)load libraries\n",
        "import json, sys, regex\n",
        "import torch\n",
        "import GPUtil\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "##----------------------------------------------------\n",
        "from transformers import *\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers import XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQWKMrXPXAkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edd98cb-574c-48ea-e356-0eb6bc7c7d43"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"your device \", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your device  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKujHwr5XAkd"
      },
      "source": [
        "\n",
        "def create_label2ind_file(file, label_col):\n",
        "\tlabels_json={}\n",
        "\t#load train_dev_test file\n",
        "\tdf = pd.read_csv(file, sep=\"\\t\")\n",
        "\tdf.head(5)\n",
        "\t#get labels and sort it A-Z\n",
        "\tlabels = df[label_col].unique()\n",
        "\tlabels.sort()\n",
        "\t#convert labels to indexes\n",
        "\tfor idx in range(0, len(labels)):\n",
        "\t\tlabels_json[labels[idx]]=idx\n",
        "\t#save labels with indexes to file\n",
        "\twith open(label2idx_file, 'w') as json_file:\n",
        "\t\tjson.dump(labels_json, json_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNZxr8m6XAkd"
      },
      "source": [
        "\n",
        "def data_prepare_BERT(file_path, lab2ind, tokenizer, content_col, label_col, MAX_LEN):\n",
        "\t# Use pandas to load dataset\n",
        "\tdf = pd.read_csv(file_path, delimiter='\\t', header=0)\n",
        "\tdf = df[df[content_col].notnull()]\n",
        "\tdf = df[df[label_col].notnull()]\n",
        "\tprint(\"Data size \", df.shape)\n",
        "\t# Create sentence and label lists\n",
        "\tsentences = df[content_col].values\n",
        "\tsentences = [\"[CLS] \" + sentence+ \" [SEP]\" for sentence in sentences]\n",
        "\tprint (\"The first sentence:\")\n",
        "\tprint (sentences[0])\n",
        "\t# Create sentence and label lists\n",
        "\tlabels = df[label_col].values\n",
        "\t#print (labels)\n",
        "\tlabels = [lab2ind[i] for i in labels]\n",
        "\t# Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "\ttokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\tprint (\"Tokenize the first sentence:\")\n",
        "\tprint (tokenized_texts[0])\n",
        "\t#print(\"Label is \", labels[0])\n",
        "\t# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "\tinput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\tprint (\"Index numbers of the first sentence:\")\n",
        "\tprint (input_ids[0])\n",
        "\t# Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "\t# ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\tpad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "\tinput_ids = pad_sequences(input_ids, maxlen=MAX_LEN+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "\tprint (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\t# Create attention masks\n",
        "\tattention_masks = []\n",
        "\t# Create a mask of 1s for each token followed by 0s for padding\n",
        "\tfor seq in input_ids:\n",
        "\t\tseq_mask = [float(i > 0) for i in seq]\n",
        "\t\tattention_masks.append(seq_mask)\n",
        "\t# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\tinputs = torch.tensor(input_ids)\n",
        "\tlabels = torch.tensor(labels)\n",
        "\tmasks = torch.tensor(attention_masks)\n",
        "\treturn inputs, labels, masks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdPCPv8VXAke"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "# def flat_accuracy(preds, labels):\n",
        "#\t  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "#\t  labels_flat = labels.flatten()\n",
        "#\t  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "def flat_pred(preds, labels):\n",
        "\tpred_flat = np.argmax(preds, axis=1).flatten()\n",
        "\tlabels_flat = labels.flatten()\n",
        "\treturn pred_flat.tolist(), labels_flat.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vyvc8JoXAke"
      },
      "source": [
        "\n",
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "\n",
        "\tmodel.train()\n",
        "\tepoch_loss = 0\n",
        "\tfor i, batch in enumerate(iterator):\n",
        "\t\t# Add batch to GPU\n",
        "\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t# Unpack the inputs from our dataloader\n",
        "\t\tinput_ids, input_mask, labels = batch\n",
        "\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\tloss, logits = outputs[:2]\n",
        "\t\t# delete used variables to free GPU memory\n",
        "\t\tdel batch, input_ids, input_mask, labels\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\telse:\n",
        "\t\t\tloss.sum().backward()\n",
        "\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\toptimizer.step()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "\t\t# optimizer.step()\n",
        "\t\tscheduler.step()\n",
        "\t# free GPU memory\n",
        "\tif device == 'cuda':\n",
        "\t\ttorch.cuda.empty_cache()\n",
        "\treturn epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjwa6a3bXAkf"
      },
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\tmodel.eval()\n",
        "\tepoch_loss = 0\n",
        "\tall_pred=[]\n",
        "\tall_label = []\n",
        "\twith torch.no_grad():\n",
        "\t\tfor i, batch in enumerate(iterator):\n",
        "\t\t\t# Add batch to GPU\n",
        "\t\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t\t# Unpack the inputs from our dataloader\n",
        "\t\t\tinput_ids, input_mask, labels = batch\n",
        "\t\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\t# delete used variables to free GPU memory\n",
        "\t\t\tdel batch, input_ids, input_mask\n",
        "\t\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\t\telse:\n",
        "\t\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\t\t# identify the predicted class for each example in the batch\n",
        "\t\t\tprobabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "\t\t\t# put all the true labels and predictions to two lists\n",
        "\t\t\tall_pred.extend(predicted)\n",
        "\t\t\tall_label.extend(labels.cpu())\n",
        "\taccuracy = accuracy_score(all_label, all_pred)\n",
        "\tf1score = f1_score(all_label, all_pred, average='macro')\n",
        "\trecall = recall_score(all_label, all_pred, average='macro')\n",
        "\tprecision = precision_score(all_label, all_pred, average='macro')\n",
        "\treport = classification_report(all_label, all_pred)\n",
        "\treturn (epoch_loss / len(iterator)), accuracy, f1score, recall, precision\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nWmg6P-XAkf"
      },
      "source": [
        "\n",
        "def fine_tuning(config):\n",
        "\t#---------------------------------------\n",
        "\tprint (\"[INFO] step (1) load train_test config file\")\n",
        "\t# config_file = open(config_file, 'r', encoding=\"utf8\")\n",
        "\t# config = json.load(config_file)\n",
        "\ttask_name = config[\"task_name\"]\n",
        "\tcontent_col = config[\"content_col\"]\n",
        "\tlabel_col = config[\"label_col\"]\n",
        "\ttrain_file = config[\"data_dir\"]+config[\"train_file\"]\n",
        "\tdev_file = config[\"data_dir\"]+config[\"dev_file\"]\n",
        "\tsortby = config[\"sortby\"]\n",
        "\tmax_seq_length= int(config[\"max_seq_length\"])\n",
        "\tbatch_size = int(config[\"batch_size\"])\n",
        "\tlr_var = float(config[\"lr\"])\n",
        "\tmodel_path = config['pretrained_model_path']\n",
        "\tnum_epochs = config['epochs'] # Number of training epochs (authors recommend between 2 and 4)\n",
        "\tglobal label2idx_file\n",
        "\tlabel2idx_file = config[\"data_dir\"]+config[\"task_name\"]+\"_labels-dict.json\"\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (2) convert labels2index\")\n",
        "\tcreate_label2ind_file(train_file, label_col)\n",
        "\tprint (label2idx_file)\n",
        "\t#---------------------------------------------------------\n",
        "\tprint (\"[INFO] step (3) check checkpoit directory and report file\")\n",
        "\tckpt_dir = config[\"data_dir\"]+task_name+\"_bert_ckpt/\"\n",
        "\treport = ckpt_dir+task_name+\"_report.tsv\"\n",
        "\tsorted_report = ckpt_dir+task_name+\"_report_sorted.tsv\"\n",
        "\tif not os.path.exists(ckpt_dir):\n",
        "\t\tos.mkdir(ckpt_dir)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (4) load label to number dictionary\")\n",
        "\tlab2ind = json.load(open(label2idx_file))\n",
        "\tprint (\"[INFO] train_file\", train_file)\n",
        "\tprint (\"[INFO] dev_file\", dev_file)\n",
        "\tprint (\"[INFO] num_epochs\", num_epochs)\n",
        "\tprint (\"[INFO] model_path\", model_path)\n",
        "\tprint (\"max_seq_length\", max_seq_length, \"batch_size\", batch_size)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (5) Use defined funtion to extract tokanize data\")\n",
        "\t# tokenizer from pre-trained BERT model\n",
        "\tprint (\"loading BERT setting\")\n",
        "\ttokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\ttrain_inputs, train_labels, train_masks = data_prepare_BERT(train_file, lab2ind, tokenizer,content_col, label_col, max_seq_length)\n",
        "\tvalidation_inputs, validation_labels, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)\n",
        "\t# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "\tmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n",
        "\t#--------------------------------------\n",
        "\tprint (\"[INFO] step (6) Create an iterator of data with torch DataLoader.\")\n",
        "#\t\t  This helps save on memory during training because, unlike a for loop,\\\n",
        "#\t\t  with an iterator the entire dataset does not need to be loaded into memory\")\n",
        "\ttrain_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "\ttrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\t#---------------------------\n",
        "\tvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "\tvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
        "\t#------------------------------------------\n",
        "\tprint (\"[INFO] step (7) run with parallel GPUs\")\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tprint(\"Run\", \"with one GPU\")\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\telse:\n",
        "\t\t\tn_gpu = torch.cuda.device_count()\n",
        "\t\t\tprint(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "\t\t\tdevice_ids = GPUtil.getAvailable(limit = 4)\n",
        "\t\t\ttorch.backends.cudnn.benchmark = True\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\t\tmodel = nn.DataParallel(model, device_ids=device_ids)\n",
        "\telse:\n",
        "\t\tprint(\"Run\", \"with CPU\")\n",
        "\t\tmodel = model\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n",
        "\tglobal max_grad_norm\n",
        "\tmax_grad_norm = 1.0\n",
        "\twarmup_proportion = 0.1\n",
        "\tnum_training_steps\t= len(train_dataloader) * num_epochs\n",
        "\tnum_warmup_steps = num_training_steps * warmup_proportion\n",
        "\t### In Transformers, optimizer and schedules are instantiated like this:\n",
        "\t# Note: AdamW is a class from the huggingface library\n",
        "\t# the 'W' stands for 'Weight Decay\"\n",
        "\toptimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False)\n",
        "\t# schedules\n",
        "\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\t# We use nn.CrossEntropyLoss() as our loss function.\n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (9) start fine_tuning\")\n",
        "\tfor epoch in trange(num_epochs, desc=\"Epoch\"):\n",
        "\t\ttrain_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\n",
        "\t\tval_loss, val_acc, val_f1, val_recall, val_precision = evaluate(model, validation_dataloader, criterion)\n",
        "# \t\tprint (train_loss, val_acc)\n",
        "\t\t# Create checkpoint at end of each epoch\n",
        "\t\tif not os.path.exists(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/'): os.mkdir(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tmodel.save_pretrained(ckpt_dir+ 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tepoch_eval_results = {\"epoch_num\":int(epoch + 1),\"train_loss\":train_loss,\n",
        "\t\t\t\t\t  \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1,\"lr\":lr_var }\n",
        "\t\twith open(report,\"a\") as fOut:\n",
        "\t\t\tfOut.write(json.dumps(epoch_eval_results)+\"\\n\")\n",
        "\t\t\tfOut.flush()\n",
        "\t\t#------------------------------------\n",
        "\t\treport_df = pd.read_json(report, orient='records', lines=True)\n",
        "\t\treport_df.sort_values(by=[sortby],ascending=False, inplace=True)\n",
        "\t\treport_df.to_csv(sorted_report,sep=\"\\t\",index=False)\n",
        "\treturn report_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDnDyOFHiZvk"
      },
      "source": [
        "# Run fine-tuning for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol6LaSAAXAki"
      },
      "source": [
        "\n",
        "config={\"task_name\": \"AJGT_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./AJGT/\", #data directory\n",
        "             \"train_file\": \"UBC_AJGT_final_shuffled_train.tsv\", #train file path\n",
        "             \"dev_file\": \"UBC_AJGT_final_shuffled_test.tsv\", #dev file path or test file path\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"epochs\": 36, #number of epochs\n",
        "             \"content_col\": \"content\", #text column\n",
        "             \"label_col\": \"label\", #label column\n",
        "             \"lr\": 2e-06, #learning rate\n",
        "              \"max_seq_length\": 128, #max sequance length\n",
        "              \"batch_size\": 32, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq9kOFoaXAkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed58b19-cc70-4fe9-e9c0-932eec736aee"
      },
      "source": [
        "report_df = fine_tuning(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading file tokenizer.json\n",
            "loading configuration file MARBERT_pytorch_verison/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (1) load train_test config file\n",
            "[INFO] step (2) convert labels2index\n",
            "./AJGT/AJGT_MARBERT_labels-dict.json\n",
            "[INFO] step (3) check checkpoit directory and report file\n",
            "[INFO] step (4) load label to number dictionary\n",
            "[INFO] train_file ./AJGT/UBC_AJGT_final_shuffled_train.tsv\n",
            "[INFO] dev_file ./AJGT/UBC_AJGT_final_shuffled_test.tsv\n",
            "[INFO] num_epochs 36\n",
            "[INFO] model_path MARBERT_pytorch_verison\n",
            "max_seq_length 128 batch_size 32\n",
            "[INFO] step (5) Use defined funtion to extract tokanize data\n",
            "loading BERT setting\n",
            "Data size  (1440, 2)\n",
            "The first sentence:\n",
            "[CLS]  اللهم  ارزقنا  فعل  الخيرات  وترك  المنكرات [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'اللهم', 'ارزقنا', 'فعل', 'الخيرات', 'وترك', 'المنكرات', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 1983, 4246, 4176, 11736, 7872, 28445, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2  1983  4246  4176 11736  7872 28445     3     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n",
            "Data size  (360, 2)\n",
            "The first sentence:\n",
            "[CLS]  فما  اجمل  هذا  الدين [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file MARBERT_pytorch_verison/config.json\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file MARBERT_pytorch_verison/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'فما', 'اجمل', 'هذا', 'الدين', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 3880, 2605, 2158, 2926, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [   2 3880 2605 2158 2926    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MARBERT_pytorch_verison and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (6) Create an iterator of data with torch DataLoader.\n",
            "[INFO] step (7) run with parallel GPUs\n",
            "Run with one GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (8) set Parameters, schedules, and loss function\n",
            "[INFO] step (9) start fine_tuning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/36 [00:00<?, ?it/s]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_1/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_1/model.safetensors\n",
            "Epoch:   3%|▎         | 1/36 [00:37<21:40, 37.16s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_2/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_2/model.safetensors\n",
            "Epoch:   6%|▌         | 2/36 [01:12<20:26, 36.08s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_3/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_3/model.safetensors\n",
            "Epoch:   8%|▊         | 3/36 [01:47<19:32, 35.54s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_4/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_4/model.safetensors\n",
            "Epoch:  11%|█         | 4/36 [02:24<19:14, 36.07s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_5/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_5/model.safetensors\n",
            "Epoch:  14%|█▍        | 5/36 [03:01<18:46, 36.35s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_6/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_6/model.safetensors\n",
            "Epoch:  17%|█▋        | 6/36 [03:35<17:53, 35.78s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_7/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_7/model.safetensors\n",
            "Epoch:  19%|█▉        | 7/36 [04:10<17:07, 35.42s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_8/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_8/model.safetensors\n",
            "Epoch:  22%|██▏       | 8/36 [04:45<16:26, 35.22s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_9/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_9/model.safetensors\n",
            "Epoch:  25%|██▌       | 9/36 [05:24<16:28, 36.62s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_10/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_10/model.safetensors\n",
            "Epoch:  28%|██▊       | 10/36 [06:09<16:58, 39.16s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_11/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_11/model.safetensors\n",
            "Epoch:  31%|███       | 11/36 [06:54<17:04, 40.97s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_12/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_12/model.safetensors\n",
            "Epoch:  33%|███▎      | 12/36 [07:34<16:14, 40.60s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_13/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_13/model.safetensors\n",
            "Epoch:  36%|███▌      | 13/36 [08:20<16:09, 42.15s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_14/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_14/model.safetensors\n",
            "Epoch:  39%|███▉      | 14/36 [09:12<16:31, 45.08s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_15/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_15/model.safetensors\n",
            "Epoch:  42%|████▏     | 15/36 [10:11<17:16, 49.36s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_16/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_16/model.safetensors\n",
            "Epoch:  44%|████▍     | 16/36 [10:49<15:21, 46.06s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_17/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_17/model.safetensors\n",
            "Epoch:  47%|████▋     | 17/36 [11:26<13:41, 43.21s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_18/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_18/model.safetensors\n",
            "Epoch:  50%|█████     | 18/36 [12:04<12:29, 41.63s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_19/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_19/model.safetensors\n",
            "Epoch:  53%|█████▎    | 19/36 [12:59<12:58, 45.82s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_20/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_20/model.safetensors\n",
            "Epoch:  56%|█████▌    | 20/36 [13:55<13:01, 48.84s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_21/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_21/model.safetensors\n",
            "Epoch:  58%|█████▊    | 21/36 [14:33<11:23, 45.56s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_22/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_22/model.safetensors\n",
            "Epoch:  61%|██████    | 22/36 [15:13<10:15, 43.96s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_23/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_23/model.safetensors\n",
            "Epoch:  64%|██████▍   | 23/36 [15:59<09:39, 44.57s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_24/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_24/model.safetensors\n",
            "Epoch:  67%|██████▋   | 24/36 [16:56<09:36, 48.03s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_25/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_25/model.safetensors\n",
            "Epoch:  69%|██████▉   | 25/36 [17:34<08:15, 45.04s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_26/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_26/model.safetensors\n",
            "Epoch:  72%|███████▏  | 26/36 [18:21<07:36, 45.67s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_27/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_27/model.safetensors\n",
            "Epoch:  75%|███████▌  | 27/36 [19:06<06:49, 45.45s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_28/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_28/model.safetensors\n",
            "Epoch:  78%|███████▊  | 28/36 [19:44<05:46, 43.36s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_29/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_29/model.safetensors\n",
            "Epoch:  81%|████████  | 29/36 [20:46<05:42, 48.95s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_30/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_30/model.safetensors\n",
            "Epoch:  83%|████████▎ | 30/36 [21:34<04:50, 48.49s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_31/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_31/model.safetensors\n",
            "Epoch:  86%|████████▌ | 31/36 [22:09<03:42, 44.51s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_32/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_32/model.safetensors\n",
            "Epoch:  89%|████████▉ | 32/36 [22:55<03:00, 45.12s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_33/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_33/model.safetensors\n",
            "Epoch:  92%|█████████▏| 33/36 [23:35<02:10, 43.54s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_34/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_34/model.safetensors\n",
            "Epoch:  94%|█████████▍| 34/36 [24:19<01:27, 43.73s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_35/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_35/model.safetensors\n",
            "Epoch:  97%|█████████▋| 35/36 [25:10<00:45, 45.74s/it]Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_36/config.json\n",
            "Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_36/model.safetensors\n",
            "Epoch: 100%|██████████| 36/36 [25:45<00:00, 42.94s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kCxZJAPOEGLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2218c1-bee6-4723-e490-da6f395d8030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V9ZOdVNXAkk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "200ef484-b0e0-4e0a-82f2-6e008e8b83a8"
      },
      "source": [
        "report_df.head(36)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    epoch_num  train_loss   val_acc  val_recall  val_precision    val_f1  \\\n",
              "18         19    0.007903  0.961111    0.960965       0.960965  0.960965   \n",
              "25         26    0.004366  0.961111    0.960965       0.960965  0.960965   \n",
              "15         16    0.007901  0.961111    0.960965       0.960965  0.960965   \n",
              "13         14    0.009370  0.961111    0.960965       0.960965  0.960965   \n",
              "12         13    0.011037  0.958333    0.958348       0.958050  0.958191   \n",
              "11         12    0.011980  0.958333    0.958007       0.958333  0.958163   \n",
              "10         11    0.013332  0.958333    0.957666       0.958732  0.958131   \n",
              "30         31    0.002933  0.955556    0.955389       0.955389  0.955389   \n",
              "9          10    0.015830  0.955556    0.954707       0.956177  0.955323   \n",
              "20         21    0.008320  0.955556    0.955730       0.955166  0.955418   \n",
              "21         22    0.003876  0.955556    0.955730       0.955166  0.955418   \n",
              "32         33    0.003016  0.952778    0.953112       0.952313  0.952646   \n",
              "33         34    0.002932  0.952778    0.953112       0.952313  0.952646   \n",
              "3           4    0.096735  0.952778    0.952090       0.953142  0.952549   \n",
              "14         15    0.008332  0.952778    0.952090       0.953142  0.952549   \n",
              "24         25    0.003616  0.952778    0.953112       0.952313  0.952646   \n",
              "23         24    0.003938  0.952778    0.953112       0.952313  0.952646   \n",
              "22         23    0.003810  0.952778    0.953112       0.952313  0.952646   \n",
              "31         32    0.002682  0.950000    0.950153       0.949596  0.949845   \n",
              "34         35    0.002686  0.950000    0.950153       0.949596  0.949845   \n",
              "35         36    0.002692  0.950000    0.950153       0.949596  0.949845   \n",
              "8           9    0.024045  0.950000    0.950835       0.949498  0.949901   \n",
              "7           8    0.026528  0.950000    0.949131       0.950583  0.949738   \n",
              "2           3    0.183169  0.947222    0.946172       0.948046  0.946924   \n",
              "19         20    0.004151  0.944444    0.943214       0.945533  0.944106   \n",
              "16         17    0.007509  0.944444    0.942873       0.946191  0.944056   \n",
              "26         27    0.003010  0.944444    0.943214       0.945533  0.944106   \n",
              "27         28    0.003740  0.944444    0.943214       0.945533  0.944106   \n",
              "28         29    0.002950  0.944444    0.943214       0.945533  0.944106   \n",
              "29         30    0.002756  0.944444    0.943214       0.945533  0.944106   \n",
              "6           7    0.027567  0.944444    0.943555       0.944988  0.944153   \n",
              "17         18    0.008578  0.938889    0.940364       0.938733  0.938821   \n",
              "5           6    0.038023  0.936111    0.933997       0.938933  0.935570   \n",
              "4           5    0.050229  0.936111    0.934338       0.938125  0.935634   \n",
              "1           2    0.497141  0.905556    0.905883       0.904980  0.905319   \n",
              "0           1    0.679298  0.863889    0.866616       0.865970  0.863879   \n",
              "\n",
              "          lr  \n",
              "18  0.000002  \n",
              "25  0.000002  \n",
              "15  0.000002  \n",
              "13  0.000002  \n",
              "12  0.000002  \n",
              "11  0.000002  \n",
              "10  0.000002  \n",
              "30  0.000002  \n",
              "9   0.000002  \n",
              "20  0.000002  \n",
              "21  0.000002  \n",
              "32  0.000002  \n",
              "33  0.000002  \n",
              "3   0.000002  \n",
              "14  0.000002  \n",
              "24  0.000002  \n",
              "23  0.000002  \n",
              "22  0.000002  \n",
              "31  0.000002  \n",
              "34  0.000002  \n",
              "35  0.000002  \n",
              "8   0.000002  \n",
              "7   0.000002  \n",
              "2   0.000002  \n",
              "19  0.000002  \n",
              "16  0.000002  \n",
              "26  0.000002  \n",
              "27  0.000002  \n",
              "28  0.000002  \n",
              "29  0.000002  \n",
              "6   0.000002  \n",
              "17  0.000002  \n",
              "5   0.000002  \n",
              "4   0.000002  \n",
              "1   0.000002  \n",
              "0   0.000002  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f66949f-26a3-402d-949e-4f04d3cfa51e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch_num</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>0.007903</td>\n",
              "      <td>0.961111</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>0.004366</td>\n",
              "      <td>0.961111</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>0.007901</td>\n",
              "      <td>0.961111</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>0.009370</td>\n",
              "      <td>0.961111</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.960965</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>0.011037</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958348</td>\n",
              "      <td>0.958050</td>\n",
              "      <td>0.958191</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>0.011980</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958007</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958163</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>0.013332</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.957666</td>\n",
              "      <td>0.958732</td>\n",
              "      <td>0.958131</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>0.002933</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.955389</td>\n",
              "      <td>0.955389</td>\n",
              "      <td>0.955389</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>0.015830</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.954707</td>\n",
              "      <td>0.956177</td>\n",
              "      <td>0.955323</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>0.008320</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.955730</td>\n",
              "      <td>0.955166</td>\n",
              "      <td>0.955418</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>0.003876</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.955730</td>\n",
              "      <td>0.955166</td>\n",
              "      <td>0.955418</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>0.003016</td>\n",
              "      <td>0.952778</td>\n",
              "      <td>0.953112</td>\n",
              "      <td>0.952313</td>\n",
              "      <td>0.952646</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>0.002932</td>\n",
              "      <td>0.952778</td>\n",
              "      <td>0.953112</td>\n",
              "      <td>0.952313</td>\n",
              "      <td>0.952646</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.096735</td>\n",
              "      <td>0.952778</td>\n",
              "      <td>0.952090</td>\n",
              "      <td>0.953142</td>\n",
              "      <td>0.952549</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>0.008332</td>\n",
              "      <td>0.952778</td>\n",
              "      <td>0.952090</td>\n",
              "      <td>0.953142</td>\n",
              "      <td>0.952549</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>0.003616</td>\n",
              "      <td>0.952778</td>\n",
              "      <td>0.953112</td>\n",
              "      <td>0.952313</td>\n",
              "      <td>0.952646</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>0.003938</td>\n",
              "      <td>0.952778</td>\n",
              "      <td>0.953112</td>\n",
              "      <td>0.952313</td>\n",
              "      <td>0.952646</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.952778</td>\n",
              "      <td>0.953112</td>\n",
              "      <td>0.952313</td>\n",
              "      <td>0.952646</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.950153</td>\n",
              "      <td>0.949596</td>\n",
              "      <td>0.949845</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>0.002686</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.950153</td>\n",
              "      <td>0.949596</td>\n",
              "      <td>0.949845</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>0.002692</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.950153</td>\n",
              "      <td>0.949596</td>\n",
              "      <td>0.949845</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>0.024045</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.950835</td>\n",
              "      <td>0.949498</td>\n",
              "      <td>0.949901</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0.026528</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.949131</td>\n",
              "      <td>0.950583</td>\n",
              "      <td>0.949738</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.183169</td>\n",
              "      <td>0.947222</td>\n",
              "      <td>0.946172</td>\n",
              "      <td>0.948046</td>\n",
              "      <td>0.946924</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>0.004151</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.943214</td>\n",
              "      <td>0.945533</td>\n",
              "      <td>0.944106</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>0.007509</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.942873</td>\n",
              "      <td>0.946191</td>\n",
              "      <td>0.944056</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>0.003010</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.943214</td>\n",
              "      <td>0.945533</td>\n",
              "      <td>0.944106</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>0.003740</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.943214</td>\n",
              "      <td>0.945533</td>\n",
              "      <td>0.944106</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>0.002950</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.943214</td>\n",
              "      <td>0.945533</td>\n",
              "      <td>0.944106</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>0.002756</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.943214</td>\n",
              "      <td>0.945533</td>\n",
              "      <td>0.944106</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0.027567</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.943555</td>\n",
              "      <td>0.944988</td>\n",
              "      <td>0.944153</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>0.008578</td>\n",
              "      <td>0.938889</td>\n",
              "      <td>0.940364</td>\n",
              "      <td>0.938733</td>\n",
              "      <td>0.938821</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0.038023</td>\n",
              "      <td>0.936111</td>\n",
              "      <td>0.933997</td>\n",
              "      <td>0.938933</td>\n",
              "      <td>0.935570</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.050229</td>\n",
              "      <td>0.936111</td>\n",
              "      <td>0.934338</td>\n",
              "      <td>0.938125</td>\n",
              "      <td>0.935634</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.497141</td>\n",
              "      <td>0.905556</td>\n",
              "      <td>0.905883</td>\n",
              "      <td>0.904980</td>\n",
              "      <td>0.905319</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.679298</td>\n",
              "      <td>0.863889</td>\n",
              "      <td>0.866616</td>\n",
              "      <td>0.865970</td>\n",
              "      <td>0.863879</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f66949f-26a3-402d-949e-4f04d3cfa51e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f66949f-26a3-402d-949e-4f04d3cfa51e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f66949f-26a3-402d-949e-4f04d3cfa51e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6443a34-382b-400e-9c64-29666a9ad938\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6443a34-382b-400e-9c64-29666a9ad938')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6443a34-382b-400e-9c64-29666a9ad938 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "report_df",
              "summary": "{\n  \"name\": \"report_df\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"epoch_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 1,\n        \"max\": 36,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          1,\n          4,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13838517618059973,\n        \"min\": 0.0026818061325070003,\n        \"max\": 0.6792977041668361,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          0.6792977041668361,\n          0.09673481674657901,\n          0.0030104792599250003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01750535317469275,\n        \"min\": 0.863888888888888,\n        \"max\": 0.961111111111111,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.9472222222222221,\n          0.961111111111111,\n          0.9055555555555551\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.017188330429251598,\n        \"min\": 0.8666160661730531,\n        \"max\": 0.9609653334985591,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.9609653334985591,\n          0.933997335729111,\n          0.9435546330431551\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.017123273108011244,\n        \"min\": 0.8659698169223151,\n        \"max\": 0.9609653334985591,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.9609653334985591,\n          0.9389334503070561,\n          0.944988344988344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01750764881732696,\n        \"min\": 0.8638794360719491,\n        \"max\": 0.9609653334985591,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.9609653334985591,\n          0.9355697177673501,\n          0.9441531444882251\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.295240995603243e-22,\n        \"min\": 2e-06,\n        \"max\": 2e-06,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoRMJ72mhK9W"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ckpt_dir = \"/content/AJGT/AJGT_MARBERT_bert_ckpt\"\n",
        "contents = os.listdir(ckpt_dir)\n",
        "print(contents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ap_B3DhLwwX",
        "outputId": "8a37711f-bf40-48c6-bb2e-652129d8cecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['model_18', 'model_29', 'model_31', 'model_23', 'model_5', 'model_35', 'model_10', 'model_36', 'model_14', 'model_30', 'model_12', 'model_27', 'model_2', 'model_20', 'model_1', 'AJGT_MARBERT_report.tsv', 'model_22', 'model_15', 'model_21', 'model_7', 'model_3', 'model_13', 'model_34', 'model_25', 'model_24', 'model_11', 'model_8', 'model_17', 'model_6', 'model_33', 'model_19', 'model_16', 'model_9', 'AJGT_MARBERT_report_sorted.tsv', 'model_28', 'model_26', 'model_32', 'model_4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ckpt_dir = \"/content/AJGT/AJGT_MARBERT_bert_ckpt\"\n",
        "for subdir in os.listdir(ckpt_dir):\n",
        "    subdir_path = os.path.join(ckpt_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "        print(f\"Contents of {subdir}:\")\n",
        "        print(os.listdir(subdir_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQQt6stBMI52",
        "outputId": "646de88b-071b-4253-a68a-91a12cf85b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of model_18:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_29:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_31:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_23:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_5:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_35:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_10:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_36:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_14:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_30:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_12:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_27:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_2:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_20:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_1:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_22:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_15:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_21:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_7:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_3:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_13:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_34:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_25:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_24:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_11:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_8:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_17:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_6:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_33:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_19:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_16:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_9:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_28:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_26:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_32:\n",
            "['model.safetensors', 'config.json']\n",
            "Contents of model_4:\n",
            "['model.safetensors', 'config.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "ckpt_dir = \"/content/AJGT/AJGT_MARBERT_bert_ckpt\"\n",
        "\n",
        "# Iterate over the subdirectories and load the model\n",
        "for subdir in os.listdir(ckpt_dir):\n",
        "    subdir_path = os.path.join(ckpt_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "        # Load the model\n",
        "        model_path = os.path.join(subdir_path, \"model.safetensors\")\n",
        "        model = BertForSequenceClassification.from_pretrained(subdir_path)\n",
        "\n",
        "        # Check if the model loaded successfully\n",
        "        if model:\n",
        "            print(f\"Model loaded successfully from {model_path}\")\n",
        "        else:\n",
        "            print(f\"Failed to load model from {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FH02VlpMXj8",
        "outputId": "2a104d34-8bf1-4808-f58c-f84889ab7a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_18/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_18/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_18.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_29/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_29/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_18/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_29.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_31/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_31/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_29/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_31.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_23/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_23/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_31/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_23.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_5/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_5/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_23/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_35/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_35/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_5/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_35.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_10/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_10/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_35/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_10.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_36/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_36/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_10/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_36.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_14/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_14/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_36/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_14.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_30/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_30/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_14/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_30.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_12/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_12/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_30/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_12.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_27/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_27/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_12/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_27.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_2/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_2/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_27/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_20/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_20/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_2/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_20.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_1/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_20/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_22/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_22/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_1/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_22.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_15/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_15/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_22/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_15.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_21/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_21/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_15/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_21.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_7/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_7/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_21/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_7.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_3/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_3/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_7/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_13/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_13/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_3/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_13.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_34/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_34/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_13/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_34.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_25/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_25/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_34/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_25.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_24/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_24/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_25/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_24.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_11/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_11/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_24/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_11.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_8/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_8/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_11/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_8.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_17/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_17/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_8/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_17.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_6/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_6/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_17/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_33/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_33/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_6/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_33.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_19/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_19/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_33/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_19.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_16/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_16/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_19/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_16.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_9/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_9/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_16/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_9.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_28/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_28/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_9/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_28.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_26/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_26/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_28/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_26.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_32/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_32/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_26/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_32.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_4/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_4/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_32/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_4.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/AJGT/AJGT_MARBERT_bert_ckpt/model_4/model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('MARBERT_pytorch_verison')\n",
        "\n",
        "# Load model\n",
        "model_path = \"/content/AJGT/AJGT_MARBERT_bert_ckpt/model_36\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.to(device)\n",
        "\n",
        "# Assuming you have a new dataset in an Excel file named \"new_data.xlsx\"\n",
        "new_data_file = \"/content/drive/My Drive/my grad/new_data.xlsx\"\n",
        "\n",
        "# Check the column names in the DataFrame\n",
        "new_data = pd.read_excel(new_data_file)\n",
        "print(new_data.columns)\n",
        "\n",
        "# Tokenize and prepare the new dataset\n",
        "encoded_data = tokenizer(new_data['content'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "input_ids = encoded_data['input_ids']\n",
        "attention_masks = encoded_data['attention_mask']\n",
        "dataset = TensorDataset(input_ids, attention_masks)\n",
        "\n",
        "# Define dataloader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "# Perform prediction\n",
        "predictions = []\n",
        "model.eval()\n",
        "for batch in dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(*batch)\n",
        "    logits = outputs[0]\n",
        "    predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
        "\n",
        "# Now, `predictions` contains the predicted labels for your new dataset\n"
      ],
      "metadata": {
        "id": "cDczW6L1RBRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a45ae2-8a7a-468e-9823-f1eaafadffff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading file tokenizer.json\n",
            "loading configuration file MARBERT_pytorch_verison/config.json\n",
            "loading configuration file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_36/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"MARBERT_pytorch_verison\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/AJGT/AJGT_MARBERT_bert_ckpt/model_36/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/AJGT/AJGT_MARBERT_bert_ckpt/model_36.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['content'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform prediction\n",
        "predictions = []\n",
        "model.eval()\n",
        "for batch in dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(*batch)\n",
        "    logits = outputs[0]\n",
        "    predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
        "\n",
        "# Now, `predictions` contains the predicted labels for your new dataset\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions:\")\n",
        "for idx, label in enumerate(predictions):\n",
        "    print(f\"Sample {idx + 1}: Predicted Label = {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "48lyZjqA9hEC",
        "outputId": "90d1743f-4e60-43f6-bcbd-fb4fe0c13ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "Sample 1: Predicted Label = 0\n",
            "Sample 2: Predicted Label = 0\n",
            "Sample 3: Predicted Label = 1\n",
            "Sample 4: Predicted Label = 0\n",
            "Sample 5: Predicted Label = 0\n",
            "Sample 6: Predicted Label = 1\n",
            "Sample 7: Predicted Label = 0\n",
            "Sample 8: Predicted Label = 0\n",
            "Sample 9: Predicted Label = 1\n",
            "Sample 10: Predicted Label = 1\n",
            "Sample 11: Predicted Label = 0\n",
            "Sample 12: Predicted Label = 0\n",
            "Sample 13: Predicted Label = 0\n",
            "Sample 14: Predicted Label = 0\n",
            "Sample 15: Predicted Label = 0\n",
            "Sample 16: Predicted Label = 1\n",
            "Sample 17: Predicted Label = 0\n",
            "Sample 18: Predicted Label = 0\n",
            "Sample 19: Predicted Label = 0\n",
            "Sample 20: Predicted Label = 1\n",
            "Sample 21: Predicted Label = 1\n",
            "Sample 22: Predicted Label = 1\n",
            "Sample 23: Predicted Label = 1\n",
            "Sample 24: Predicted Label = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping of predicted indices to negative or positive labels\n",
        "label_mapping = {\n",
        "    0: \"Negative\",\n",
        "    1: \"Positive\"\n",
        "}\n",
        "\n",
        "# Perform prediction\n",
        "predictions = []\n",
        "model.eval()\n",
        "for batch in dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(*batch)\n",
        "    logits = outputs[0]\n",
        "    predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
        "\n",
        "# Now, `predictions` contains the predicted labels for your new dataset\n",
        "\n",
        "# Convert predicted indices to negative or positive labels using the mapping\n",
        "converted_predictions = [label_mapping[idx] for idx in predictions]\n",
        "\n",
        "# Print the converted predictions\n",
        "print(\"Predictions:\")\n",
        "for idx, label in enumerate(converted_predictions):\n",
        "    print(f\"Sample {idx + 1}: Predicted Label = {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp8lDl0vB3Qc",
        "outputId": "1e66c6ab-4454-4257-abd5-261739a9995b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "Sample 1: Predicted Label = Negative\n",
            "Sample 2: Predicted Label = Negative\n",
            "Sample 3: Predicted Label = Positive\n",
            "Sample 4: Predicted Label = Negative\n",
            "Sample 5: Predicted Label = Negative\n",
            "Sample 6: Predicted Label = Positive\n",
            "Sample 7: Predicted Label = Negative\n",
            "Sample 8: Predicted Label = Negative\n",
            "Sample 9: Predicted Label = Positive\n",
            "Sample 10: Predicted Label = Positive\n",
            "Sample 11: Predicted Label = Negative\n",
            "Sample 12: Predicted Label = Negative\n",
            "Sample 13: Predicted Label = Negative\n",
            "Sample 14: Predicted Label = Negative\n",
            "Sample 15: Predicted Label = Negative\n",
            "Sample 16: Predicted Label = Positive\n",
            "Sample 17: Predicted Label = Negative\n",
            "Sample 18: Predicted Label = Negative\n",
            "Sample 19: Predicted Label = Negative\n",
            "Sample 20: Predicted Label = Positive\n",
            "Sample 21: Predicted Label = Positive\n",
            "Sample 22: Predicted Label = Positive\n",
            "Sample 23: Predicted Label = Positive\n",
            "Sample 24: Predicted Label = Positive\n"
          ]
        }
      ]
    }
  ]
}